{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Install required packages\n",
        "!pip install -q transformers datasets accelerate evaluate rouge_score sentencepiece torch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\r\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "import evaluate\n",
        "\n",
        "# Check GPU\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "GPU: NVIDIA H100 80GB HBM3\n",
            "GPU Memory: 85.02 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load and Explore Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Load dataset\n",
        "dataset = load_dataset(\"kdave/Indian_Financial_News\")\n",
        "\n",
        "# Check structure\n",
        "print(dataset)\n",
        "print(f\"\\nTotal rows: {len(dataset['train'])}\")\n",
        "print(\"\\nColumns:\", dataset['train'].column_names)\n",
        "print(\"\\nSample row:\")\n",
        "print(f\"Content: {dataset['train'][0]['Content'][:200]}...\")\n",
        "print(f\"Summary: {dataset['train'][0]['Summary']}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d459f6b201fb4d0f82f046c3b9033c20",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "README.md: 0.00B [00:00, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e4a49801b4a54ea09f54cc0ed3b5d507",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "training_data_26000.csv:   0%|          | 0.00/115M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b1dbdc37b2d2433aa641697fba80b85b",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "Generating train split:   0%|          | 0/26961 [00:00<?, ? examples/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['URL', 'Content', 'Summary', 'Sentiment'],\n",
            "        num_rows: 26961\n",
            "    })\n",
            "})\n",
            "\n",
            "Total rows: 26961\n",
            "\n",
            "Columns: ['URL', 'Content', 'Summary', 'Sentiment']\n",
            "\n",
            "Sample row:\n",
            "Content: US consumer spending dropped by a record in April as the COVID-19 pandemic undercut demand, buttressing expectations that the economy could contract in the second quarter at its steepest pace since th...\n",
            "Summary: consumer spending plunges 13.6 percent in April. that was the biggest drop since the government started tracking series in 1959. consumer spending accounts for more than two-thirds of economic activity. economists polled by Reuters had forecast consumer spending plummeting 12.6 percent. a spokesman for the u.s. government said the data was not available.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Clean and Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "def clean_text(example):\n",
        "    \"\"\"Clean Content and Summary: remove extra spaces, newlines, special chars\"\"\"\n",
        "    # Clean content\n",
        "    text = example['Content']\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Multiple spaces to single\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?%-]', '', text)  # Keep basic punctuation\n",
        "    example['Content'] = text.strip()\n",
        "\n",
        "    # Clean summary\n",
        "    summary = example['Summary']\n",
        "    summary = re.sub(r'\\s+', ' ', summary)\n",
        "    example['Summary'] = summary.strip()\n",
        "    \n",
        "    return example\n",
        "\n",
        "# Clean dataset\n",
        "dataset = dataset.map(clean_text)\n",
        "\n",
        "# Split into train (95%) and test (5%) for final evaluation\n",
        "# We use all train data for training (no validation during training)\n",
        "dataset = dataset['train'].train_test_split(test_size=0.05, seed=42)\n",
        "print(f\"\\nTrain: {len(dataset['train'])} samples\")\n",
        "print(f\"Test (for final evaluation): {len(dataset['test'])} samples\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6515f4d4ccf46e69e09e9b5ec89cdcc",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "Map:   0%|          | 0/26961 [00:00<?, ? examples/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train: 25612 samples\n",
            "Test (for final evaluation): 1349 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Load FLAN-T5 Model and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Load FLAN-T5 base model (you can also use flan-t5-large for better quality)\n",
        "model_name = \"google/flan-t5-base\"\n",
        "# model_name = \"google/flan-t5-large\"  # Uncomment for larger model\n",
        "\n",
        "print(f\"Loading {model_name}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "print(f\"\\nModel loaded: {model_name}\")\n",
        "print(f\"Model parameters: {model.num_parameters() / 1e6:.2f}M\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading google/flan-t5-base...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9d8f1aa1c8ef4767ad3aaf9d046bc219",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "tokenizer_config.json: 0.00B [00:00, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6ea48ce603404ffb91ca8211b58dc8c8",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d4a389a64a849f1b46e0a13461a56b1",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "tokenizer.json: 0.00B [00:00, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b46beec22bd44b64bd3b0e9fac70bfb9",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "special_tokens_map.json: 0.00B [00:00, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9356872903da4c419df8dfdb798167d9",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "config.json: 0.00B [00:00, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f5d15c43e3634b8f8efd1961fdb879c9",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4381670813b041278862408955edbe7b",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model loaded: google/flan-t5-base\n",
            "Model parameters: 247.58M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Tokenize Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# FLAN-T5 specific preprocessing\n",
        "# Add task prefix for better performance\n",
        "def preprocess_flan_t5(examples):\n",
        "    \"\"\"Preprocess for FLAN-T5 with task prefix\"\"\"\n",
        "    # Add summarization task prefix\n",
        "    inputs = [f\"summarize: {doc}\" for doc in examples['Content']]\n",
        "    \n",
        "    # Tokenize inputs\n",
        "    model_inputs = tokenizer(\n",
        "        inputs,\n",
        "        max_length=512,\n",
        "        truncation=True,\n",
        "        padding=False  # Dynamic padding in data collator\n",
        "    )\n",
        "    \n",
        "    # Tokenize targets\n",
        "    labels = tokenizer(\n",
        "        examples['Summary'],\n",
        "        max_length=128,\n",
        "        truncation=True,\n",
        "        padding=False  # Dynamic padding in data collator\n",
        "    )\n",
        "    \n",
        "    model_inputs['labels'] = labels['input_ids']\n",
        "    return model_inputs\n",
        "\n",
        "# Tokenize dataset\n",
        "print(\"Tokenizing dataset...\")\n",
        "tokenized_data = dataset.map(\n",
        "    preprocess_flan_t5,\n",
        "    batched=True,\n",
        "    remove_columns=['Content', 'Summary', 'URL', 'Sentiment'],\n",
        "    desc=\"Tokenizing\"\n",
        ")\n",
        "\n",
        "print(\"\\nTokenization complete!\")\n",
        "print(f\"Train samples: {len(tokenized_data['train'])}\")\n",
        "print(f\"Test samples: {len(tokenized_data['test'])}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing dataset...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6b10b78a21cb46dd8a0a57fd2e8ab5fb",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "Tokenizing:   0%|          | 0/25612 [00:00<?, ? examples/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a0449d4f390c481e9b6236d83c58b6b9",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "Tokenizing:   0%|          | 0/1349 [00:00<?, ? examples/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokenization complete!\n",
            "Train samples: 25612\n",
            "Test samples: 1349\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Setup Training Arguments (Optimized for H100)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Data collator for dynamic padding\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    padding=True\n",
        ")\n",
        "\n",
        "# Training arguments optimized for H100\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./flan-t5-financial\",\n",
        "    \n",
        "    # Training strategy\n",
        "    eval_strategy=\"no\",  # No evaluation during training\n",
        "    save_strategy=\"epoch\",  # Save after each epoch\n",
        "    \n",
        "    # Hyperparameters\n",
        "    learning_rate=5e-5,  # Higher LR for FLAN-T5\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    \n",
        "    # Batch sizes (H100 can handle larger batches)\n",
        "    per_device_train_batch_size=16,  # Adjust based on H100 memory\n",
        "    per_device_eval_batch_size=16,\n",
        "    gradient_accumulation_steps=2,  # Effective batch size = 32\n",
        "    \n",
        "    # Performance optimization\n",
        "    bf16=True,  # H100 supports BF16\n",
        "    dataloader_num_workers=4,\n",
        "    dataloader_pin_memory=True,\n",
        "    gradient_checkpointing=False,  # H100 has enough memory\n",
        "    \n",
        "    # Logging\n",
        "    logging_steps=100,\n",
        "    logging_first_step=True,\n",
        "    report_to=\"none\",  # Disable wandb/tensorboard\n",
        "    \n",
        "    # Saving\n",
        "    save_total_limit=2,  # Keep only last 2 checkpoints\n",
        "    \n",
        "    # Generation\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=128,\n",
        "    generation_num_beams=4,\n",
        "    \n",
        "    # Misc\n",
        "    push_to_hub=False,\n",
        "    load_best_model_at_end=False,\n",
        ")\n",
        "\n",
        "print(\"Training arguments configured!\")\n",
        "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"Total training steps: {len(tokenized_data['train']) * training_args.num_train_epochs // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training arguments configured!\n",
            "Effective batch size: 32\n",
            "Total training steps: 2401\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Setup Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Load ROUGE metric\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    \"\"\"Compute ROUGE metrics for evaluation\"\"\"\n",
        "    preds, labels = eval_preds\n",
        "    \n",
        "    # Replace -100 in labels (used for padding)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    \n",
        "    # Decode predictions and labels\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    \n",
        "    # Compute ROUGE scores\n",
        "    result = rouge.compute(\n",
        "        predictions=decoded_preds,\n",
        "        references=decoded_labels,\n",
        "        use_stemmer=True\n",
        "    )\n",
        "    \n",
        "    return {\n",
        "        \"rouge1\": result[\"rouge1\"],\n",
        "        \"rouge2\": result[\"rouge2\"],\n",
        "        \"rougeL\": result[\"rougeL\"],\n",
        "    }\n",
        "\n",
        "print(\"Evaluation metrics configured!\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1652a9e9b15e4a649f2215194ad68830",
              "version_minor": 0.0,
              "version_major": 2.0
            },
            "text/plain": "Downloading builder script: 0.00B [00:00, ?B/s]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation metrics configured!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Initialize Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_data['train'],\n",
        "    eval_dataset=None,  # No evaluation during training\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized!\")\n",
        "print(f\"Model device: {next(model.parameters()).device}\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_341/1965046093.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n",
            "Detected kernel version 4.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainer initialized!\n",
            "Model device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "print(\"Starting training...\\n\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Train\n",
        "train_result = trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Training completed!\")\n",
        "print(f\"Training time: {train_result.metrics['train_runtime']:.2f} seconds\")\n",
        "print(f\"Training loss: {train_result.metrics['train_loss']:.4f}\")\n",
        "print(f\"Samples per second: {train_result.metrics['train_samples_per_second']:.2f}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "\n    <div>\n      \n      <progress value='2403' max='2403' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2403/2403 09:10, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.324900</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.791200</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.670600</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.624600</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.607500</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.594400</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.576600</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.572400</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.555000</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.528300</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.533100</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.513900</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.516400</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.512900</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.504900</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.510600</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.500800</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.489400</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.488000</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.488200</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.484500</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.477000</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.478800</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.475200</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.479500</td>\n    </tr>\n  </tbody>\n</table><p>",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Training completed!\n",
            "Training time: 552.93 seconds\n",
            "Training loss: 0.5407\n",
            "Samples per second: 138.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Final Evaluation on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Starting final evaluation on test set...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Disable metrics during evaluation\n",
        "trainer.compute_metrics = None\n",
        "\n",
        "# Run evaluation to get loss\n",
        "eval_results = trainer.evaluate(\n",
        "    eval_dataset=tokenized_data[\"test\"],\n",
        "    max_length=128,\n",
        "    num_beams=2,\n",
        ")\n",
        "\n",
        "print(f\"\\n\u2705 Evaluation Loss: {eval_results['eval_loss']:.4f}\")\n",
        "\n",
        "# Generate predictions\n",
        "print(\"\\n\u23f3 Generating predictions for ROUGE scores...\")\n",
        "predictions = trainer.predict(\n",
        "    test_dataset=tokenized_data[\"test\"],\n",
        "    max_length=128,\n",
        "    num_beams=2,\n",
        ")\n",
        "\n",
        "preds = predictions.predictions\n",
        "if isinstance(preds, tuple):\n",
        "    preds = preds[0]\n",
        "\n",
        "labels = predictions.label_ids\n",
        "\n",
        "# FIX: Clip to valid vocab range (0 to 32127 for FLAN-T5)\n",
        "vocab_size = tokenizer.vocab_size\n",
        "preds = np.clip(preds, 0, vocab_size - 1).astype(np.int64)\n",
        "labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "labels = np.clip(labels, 0, vocab_size - 1).astype(np.int64)\n",
        "\n",
        "# Decode\n",
        "decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "# Compute ROUGE\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "rouge_results = rouge.compute(\n",
        "    predictions=decoded_preds,\n",
        "    references=decoded_labels,\n",
        "    use_stemmer=True\n",
        ")\n",
        "\n",
        "# Print results\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"\ud83c\udfaf FINAL EVALUATION RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Evaluation Loss: {eval_results['eval_loss']:.4f}\")\n",
        "print(f\"ROUGE-1: {rouge_results['rouge1']:.4f}\")\n",
        "print(f\"ROUGE-2: {rouge_results['rouge2']:.4f}\")\n",
        "print(f\"ROUGE-L: {rouge_results['rougeL']:.4f}\")\n",
        "print(\"=\" * 60)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Starting final evaluation on test set...\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u2705 Evaluation Loss: 0.3855\n",
            "\n",
            "\u23f3 Generating predictions for ROUGE scores...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "\ud83c\udfaf FINAL EVALUATION RESULTS\n",
            "============================================================\n",
            "Evaluation Loss: 0.3855\n",
            "ROUGE-1: 0.5688\n",
            "ROUGE-2: 0.4397\n",
            "ROUGE-L: 0.5002\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Generate Sample Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "# Generate predictions on a few test samples\n",
        "print(\"\\nGenerating sample predictions...\\n\")\n",
        "\n",
        "test_samples = dataset['test'].select(range(3))\n",
        "\n",
        "for i, sample in enumerate(test_samples):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"SAMPLE {i+1}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Prepare input\n",
        "    input_text = f\"summarize: {sample['Content']}\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(model.device)\n",
        "    \n",
        "    # Generate summary\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=128,\n",
        "        num_beams=4,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    \n",
        "    predicted_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    print(f\"\\nOriginal Content (first 200 chars):\\n{sample['Content'][:200]}...\")\n",
        "    print(f\"\\nReference Summary:\\n{sample['Summary']}\")\n",
        "    print(f\"\\nGenerated Summary:\\n{predicted_summary}\")\n",
        "    print(f\"\\n{'='*60}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating sample predictions...\n",
            "\n",
            "\n",
            "============================================================\n",
            "SAMPLE 1\n",
            "============================================================\n",
            "\n",
            "Original Content (first 200 chars):\n",
            "The consumer durables market in India was valued at 90,000 crore in 2019, according to industry estimates, but is expected to decline to 80,000 crore due to the loss in sales during the lockdown perio...\n",
            "\n",
            "Reference Summary:\n",
            "over 56 million dishwashers were sold in india in 2019, and this is expected to rise to 70 million this year. the microwave category \u2014 which has been witnessing a flat retail volume since 2017 \u2014 is expected to touch 1,548 million this year. in-store tactics Lloyd (part of Havells) forayed into the refrigerator segment in September with 25 models priced between 10,000 and 84,990.\n",
            "\n",
            "Generated Summary:\n",
            "consumer durables market in india was valued at 90,000 crore in 2019, but is expected to decline to 80,000 crore due to the loss in sales during the lockdown period. over 56 million units of dishwashers were sold in india in 2019, and this is expected to rise to 70 million this year. microwave category which has been witnessing a flat retail volume since 2017 is expected to touch 1,548 million this year.\n",
            "\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "SAMPLE 2\n",
            "============================================================\n",
            "\n",
            "Original Content (first 200 chars):\n",
            "The need for a deeper  liquid bond market has never been felt more than now On receiving representation from Mauritius, an internal analysis was done by Sebi to evaluate the impact of new regulations ...\n",
            "\n",
            "Reference Summary:\n",
            "a deeper & liquid bond market has never been felt more than now. various regulations were modified to protect investors\u2019 interests. enforcement mechanism in sebi was also made effective. a re-analysis was done to evaluate the impact of new regulations on FPIs from Mauritius. a re-analysis was done to assess the impact of new regulations on FPIs from Mauritius.\n",
            "\n",
            "Generated Summary:\n",
            "an internal analysis was done by Sebi to evaluate the impact of new regulations on FPIs from Mauritius. coming to the pre-Covid period, various regulations were modified. some of them were rewritten to bring them in sync with the evolving market conditions. the enforcement mechanism in Sebi was also made effective. the need for a deeper liquid bond market has never been felt more than now.\n",
            "\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "SAMPLE 3\n",
            "============================================================\n",
            "\n",
            "Original Content (first 200 chars):\n",
            "When it comes to saving taxes, the Public Provident Fund PPF is one product a lot of people turn to. There are two reasons for this its tax-free yearly interest and the annual compounding. Since the P...\n",
            "\n",
            "Reference Summary:\n",
            "public provident fund (PPF) is one product a lot of people turn to for saving taxes. interest rate on PPF is set by the government every quarter based on the yield of government securities. the power of compounding works best over the long term. if you want to save tax, you can use the public provident fund as a retirement fund.\n",
            "\n",
            "Generated Summary:\n",
            "the public provident fund (PPF) is one product a lot of people turn to. the impact of compounding is huge, especially in the later years. the interest earned is backed by sovereign guarantee, it makes it a safe investment. the current interest rate for January to March 2022 is 7.1 per cent per annum. the maximum amount that can be deposited in a financial year is Rs 1.5 lakh.\n",
            "\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "import os\n",
        "\n",
        "SAVE_PATH = \"/mnt/models/flan-t5-financial-final\"\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)\n",
        "\n",
        "trainer.save_model(SAVE_PATH)\n",
        "tokenizer.save_pretrained(SAVE_PATH)\n",
        "\n",
        "print(f\"\u2705 Model saved to {SAVE_PATH}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u2705 Model saved to /mnt/models/flan-t5-financial-final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "MODEL_DIR = Path(\"/mnt/models/flan-t5-financial-final\")\n",
        "ZIP_PATH = Path(\"/mnt/models/flan-t5-financial-final.zip\")\n",
        "\n",
        "# Create zip\n",
        "shutil.make_archive(\n",
        "    base_name=str(ZIP_PATH).replace(\".zip\", \"\"),\n",
        "    format=\"zip\",\n",
        "    root_dir=MODEL_DIR\n",
        ")\n",
        "\n",
        "ZIP_PATH\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/mnt/models/flan-t5-financial-final'",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m ZIP_PATH = Path(\u001b[33m\"\u001b[39m\u001b[33m/mnt/models/flan-t5-financial-final.zip\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Create zip\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mshutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_archive\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mZIP_PATH\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.zip\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mzip\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODEL_DIR\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m ZIP_PATH\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/shutil.py:1144\u001b[39m, in \u001b[36mmake_archive\u001b[39m\u001b[34m(base_name, format, root_dir, base_dir, verbose, dry_run, owner, group, logger)\u001b[39m\n\u001b[32m   1142\u001b[39m save_cwd = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1143\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m root_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1144\u001b[39m     stmd = \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m)\u001b[49m.st_mode\n\u001b[32m   1145\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stat.S_ISDIR(stmd):\n\u001b[32m   1146\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotADirectoryError\u001b[39;00m(errno.ENOTDIR, \u001b[33m'\u001b[39m\u001b[33mNot a directory\u001b[39m\u001b[33m'\u001b[39m, root_dir)\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/mnt/models/flan-t5-financial-final'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "import os\n",
        "\n",
        "os.listdir(\"/mnt\")\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "['my-flan-t5-volume']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "import os\n",
        "\n",
        "os.listdir(\"/mnt/my-flan-t5-volume\")\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "[]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "import os\n",
        "\n",
        "model_paths = []\n",
        "\n",
        "for root, dirs, files in os.walk(\"/\"):\n",
        "    if \"config.json\" in files and (\"pytorch_model.bin\" in files or \"model.safetensors\" in files):\n",
        "        model_paths.append(root)\n",
        "\n",
        "model_paths\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "[]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "SAVE_PATH = \"/mnt/my-flan-t5-volume/flan-t5-financial-final\"\n",
        "\n",
        "model.save_pretrained(SAVE_PATH)\n",
        "tokenizer.save_pretrained(SAVE_PATH)\n",
        "\n",
        "SAVE_PATH\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m SAVE_PATH = \u001b[33m\"\u001b[39m\u001b[33m/mnt/my-flan-t5-volume/flan-t5-financial-final\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mmodel\u001b[49m.save_pretrained(SAVE_PATH)\n\u001b[32m      4\u001b[39m tokenizer.save_pretrained(SAVE_PATH)\n\u001b[32m      6\u001b[39m SAVE_PATH\n",
            "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [
        "import os\n",
        "\n",
        "for root, dirs, files in os.walk(\"/\"):\n",
        "    for d in dirs:\n",
        "        if d.startswith(\"checkpoint\"):\n",
        "            print(os.path.join(root, d))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/site-packages/openai/resources/fine_tuning/checkpoints\n",
            "/usr/local/lib/python3.12/site-packages/openai/types/fine_tuning/checkpoints\n",
            "/usr/local/lib/python3.12/site-packages/orbax/checkpoint\n",
            "/usr/local/lib/python3.12/site-packages/orbax/checkpoint/_src/checkpoint_managers\n",
            "/usr/local/lib/python3.12/site-packages/orbax/checkpoint/_src/checkpointers\n",
            "/usr/local/lib/python3.12/site-packages/sonnet/src/conformance/checkpoints\n",
            "/usr/local/lib/python3.12/site-packages/torch/distributed/checkpoint\n",
            "/usr/local/lib/python3.12/site-packages/torch/distributed/_shard/checkpoint\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}